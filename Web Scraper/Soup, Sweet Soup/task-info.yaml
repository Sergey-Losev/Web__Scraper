type: edu
files:
- name: scraper.py
  visible: true
  text: |
    import requests
    from bs4 import BeautifulSoup
    import json
    import string


    def initial_request():
        user_url = input()
        r = requests.get(user_url, headers={'Accept-Language': 'en-US,en;q=0.5'})
        if r and 'content' in r.json():
            print(r.json()['content'])
        else:
            print(f'The URL returned {r.status_code}!')


    def check_link():
        user_link = input()
        requests_link = requests.get(user_link, headers={'Accept-Language': 'en-US,en;q=0.5'})
        soup = BeautifulSoup(requests_link.content, 'html.parser')
        # check_movie = soup.find('script', type="application/ld+json").attrs
        check_movie = json.loads("".join(soup.find("script", {"type": "application/ld+json"}).contents))
        print('Invalid movie page!')
        if check_movie['@type'] == 'Movie' or check_movie['@type'] == 'TVSeries':
            print({'title': check_movie['name'], 'description': check_movie['description']})
        else:
            print('Invalid movie page!')


    def new_request():
        user_url = input('Input the URL:\n')
        r = requests.get(user_url)
        if r:
            with open('source.html', 'wb') as f:
                f.write(r.content)
            print('Content saved.')
        else:
            print(f'The URL returned {r.status_code}!')


    def nature_news():
        nature_url = 'https://www.nature.com/nature/articles'
        nature_r = requests.get(nature_url)
        soup = BeautifulSoup(nature_r.content, 'html.parser')
        news = soup.find_all('div', class_ = 'c-card__body u-display-flex u-flex-direction-column')
        list_of_news = []
        counter = 0
        for item in news:
            if item.find('span', class_ = "c-meta__type").get_text() == 'News':
                news_link = 'https://www.nature.com' + item.find('a').get('href')
                news_soup = BeautifulSoup(requests.get(news_link).content.decode('utf-8'), 'html.parser')
                title = item.find("a").get_text()
                for p in string.punctuation:
                    if p in title:
                        title = title.replace(p, '')
                title = title.replace(' ', '_')
                print(title)
                with open(f'{title}.txt', 'w', encoding='UTF-8') as f:
                    f.write(news_soup.find('article').get_text())
                # print(news_soup.find('article').get_text())
                list_of_news.append(item.find('a').text)
                counter += 1
        print(list_of_news)


    nature_news()
  learner_created: false
- name: tests.py
  visible: false
  text: |
    import glob
    import random
    import re
    import shutil
    import string

    from hstest import *

    import requests
    from furl import furl
    from bs4 import BeautifulSoup
    import os


    class NatureScraper:
        def tag_leading_to_view_article(self, tag):
            return tag.has_attr("data-track-action") and tag["data-track-action"] == "view article"

        def tag_containing_atricle_type(self, tag):
            return tag.name == "span" and tag.has_attr("data-test") and tag["data-test"] == "article.type"

        def tag_containing_article_title(self, tag):
            return tag.name == "h1" and ("article" in tag["class"][0] and "title" in tag["class"][0])

        def tag_containing_article_body(self, tag):
            return tag.name == "div" and ("article" in tag.get("class", [""])[0] and "body" in tag.get("class", [""])[0])

        def get_article_links_of_type(self, url, article_type="News"):
            origin_url = furl(url).origin
            try:
                articles_resp = requests.get(url)
            except requests.exceptions.ConnectionError:
                raise WrongAnswer(f"ConnectionError occurred when tests tried to reach the page \'{url}\'.\n"
                                  f"Please try running tests again.")
            soup = BeautifulSoup(articles_resp.text, "html.parser")
            articles = soup.find_all(self.tag_containing_atricle_type)
            articles = list(filter(lambda x: x.text.strip() == article_type, articles))
            return [furl(origin_url).add(path=x.find_parent("article").find(self.tag_leading_to_view_article).get("href")).url \
                             for x in articles]

        def get_article_title_and_content(self, url):
            article = requests.get(url)
            soup = BeautifulSoup(article.text, "html.parser")
            title = soup.find(self.tag_containing_article_title)
            content = soup.find(self.tag_containing_article_body)
            if title and content:
                return title.text.strip(), content.text.strip()


    class WebScraperTest(StageTest):
        def generate(self):
            for name in os.listdir():
                if os.path.isdir(name) and name.startswith("Page_"):
                    shutil.rmtree(name)

            return [TestCase(stdin="3\nResearch Highlight", attach=(3, "Research Highlight"), time_limit=0),
                    TestCase(stdin="1\nNews & Views", attach=(1, "News & Views"), time_limit=0),
                    TestCase(stdin="2\nNews Feature", attach=(2, "News Feature"), time_limit=0)]

        def check(self, reply, attach=None):
            n_pages, article_type = attach
            scraper = NatureScraper()
            for i in range(1, n_pages + 1):
                dirname = f"Page_{i}"
                dirname = os.path.abspath(dirname)
                if not os.path.exists(dirname):
                    return CheckResult.wrong(f"Impossible to find directory {dirname}")
                os.chdir(dirname)
                txt_files = glob.glob("*.txt")
                url = furl("https://www.nature.com/nature/articles").add({"page": str(i)})
                article_links = scraper.get_article_links_of_type(url, article_type=article_type)
                if len(txt_files) != len(article_links):
                    return CheckResult.wrong("A wrong number of files with articles was found in the directory {0}. \n"
                                             "{1} files were found, {2} files were expected.".format(dirname,
                                                                                                     len(txt_files),
                                                                                                     len(article_links)))
                if article_links:
                    random_val = random.randint(0, len(article_links)-1)
                    title, content = scraper.get_article_title_and_content(article_links[random_val])
                    content = content.strip()
                    title = f"{title.translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')}.txt"
                    title = os.path.abspath(title)
                    if not os.path.exists(title):
                        return CheckResult.wrong("A file with the title {0} was expected, but was not found.".format(title))
                    with open(title, "rb") as f:
                        try:
                            file_content = f.read().decode('utf-8').strip()
                        except UnicodeDecodeError:
                            return CheckResult.wrong("An error occurred when tests tried to read the file \"{0}\"\n"
                                                     "Please, make sure you save your file in binary format \n"
                                                     "and encode the saved data using utf-8 encoding.".format(title))

                    file_content = re.sub('[\r\n]', '', file_content)
                    content = re.sub('[\r\n]', '', content)
                    if file_content != content:
                        return CheckResult.wrong("Some of the files do not contain the expected article's body. \n"
                                                 "The tests expected the following article:\n"
                                                 f"\"{content}\"\n"
                                                 f"However, the following text was found in the file {title}:\n"
                                                 f"\"{file_content}\"")
                os.chdir("..")
                try:
                    shutil.rmtree(dirname)
                except OSError as e:
                    print(f"The following error occurred when the tests tried to remove directory {dirname}:\n"
                          f"{e}\n"
                          f"If you can, please, make it possible to remove the directory.")
            return CheckResult.correct()


    if __name__ == '__main__':
        WebScraperTest().run_tests()
  learner_created: false
- name: Page_1/Forget_Stonehenge_the_first_known_massive_monuments_are_much_older__Research_Highlights.txt
  visible: true
  learner_created: true
- name: Page_1/Forests_that_float_in_the_clouds_are_drifting_away__Research_Highlights.txt
  visible: true
  learner_created: true
- name: Page_2/Forests_that_float_in_the_clouds_are_drifting_away.txt
  visible: true
  learner_created: true
- name: Page_2/Forget_Stonehenge_the_first_known_massive_monuments_are_much_older.txt
  visible: true
  learner_created: true
feedback_link: https://hyperskill.org/projects/145/stages/785/implement#comment
status: Solved
feedback:
  message: Congratulations!
  time: Thu, 06 May 2021 06:34:32 UTC
record: 4
